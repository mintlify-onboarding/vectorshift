---
title: "Chatbot"
---

**Scenario: We want to build a customer service chatbot that answers questions about our product and embed it into the website. The main data source we want the chatbot to leverage is our website: vectorshift.ai.** 

With respect to structure, the pipeline will look very similar to the Document Search pipeline. The only difference is we need to utilize a URL loader instead of a file loader. 

At a high level, we need to create the following pipeline components:

1. A way to feed a website URL into the pipeline and embed it into a Vector database.

2. A way for the LLM to receive 1) queries from the Vector Database, 2) store conversation history, and 3) receive a question from the user.

3. A large language model is instructed to be a chatbot that answers questions from the user based on content from the website.

## Step 1 - Create a Pipeline

Open the pipeline builder: click “Build a Pipeline” within the “Overview” tab or click “Build” within the “Pipeline” tab.

![](https://lh4.googleusercontent.com/HH2YJNvx1OoB_bfNbV1ISwpTQdS5V9BosDoDRWMm6DaoGGG75Juxtm743Vux6506Jrqdni3fr6gk2u3sBDkbyFpBhCRA6X_fxpBLrtHXPhfXszR10EhKfoZLVRatn6XqCZ_MT75MhQs6UqmPWRVpryc)

## Step 2 - Feed the Website URL

Feed the website URL into the pipeline and embed into Vector database; we need to create a structure that allows the pipeline to accept a URL link, feed scraped content into a Vector database, and allow for relevant information to be queried from the Vector database.

Creating the building blocks for the above action will have the following four nodes structure below:

1. [**Text**](/vectorshift/platform/pipelines/text): place link of URL. Content in the URL contains the context for the chatbot.

2. [**URL Loader**](/vectorshift/platform/pipelines/data-loaders#url-loader): this converts the contents of the website into a format that the VectorDB can understand.

3. [**VectorDB Loader**](/vectorshift/platform/pipelines/vector-db#vectordb-loader): loads the data into the Vector Database.

4. [**Vector DB Reader**](/vectorshift/platform/pipelines/vector-db#vectordb-reader): provides an endpoint where the user can query the VectorDatabase.

Note: we connect the Vector DB loader to the “database” node of the VectorDB reader.

![](https://lh5.googleusercontent.com/SzT99Ndu0roDtdCTdnRHQAov-AGQDZBC8DPT6_yWsmS6OG7pwce_w_FrfqnSYJN1e7DRFrrjDPXugMbNrmwOGZTe87T9Wuye4VFncbZ4EUN7ZKd0K-sOesoo5ECuFJQWL187O4MPVilfVJxh20Ho34M)

Feed Website URL

## Step 3 - Design Prompts

Create a prompt that contains queries from the Vector database, store conversation history, and the question from the user, here's how to do it:

1. The [Text ](/vectorshift/platform/pipelines/text)node will serve as the “prompt” that will eventually prompt the LLM. Within this node, every time you use curly braces {}, the text within the braces will automatically appear on the left-hand side of the block. Hence, the associated data connected to the named nodes will “replace” the curly brackets when the pipeline runs.

2. The [Chat Memory](/vectorshift/platform/pipelines/chat#chat-memory) node will allow the LLM to “remember” the previous conversation history. We connect it to the text node so that every time the text is passed to the LLM as a prompt, the previous conversation is also passed through.

3. The input node here will allow the user to ask questions. We need to connect it both to the [VectorDB](/vectorshift/platform/pipelines/vector-db) Reader node (`“query”` node) and the text node (prompt for the LLM). This is because:  
   * Passing the question into the VectorDB reader will allow for the VectorDB reader to query relevant information, which is outputted and also inputted into the prompt and  
   * we need to pass the question into the prompt (text node) so the LLM receives the user input as well.

![](https://lh4.googleusercontent.com/e-3rpccHr3yNCMMzMc_Fj6ZMz9QjOPAr-uhv6Sa1jNxQIcJKvf-462934uKpOcsWUsiycrPd9eIOvgq-mI9PHpax9SyQP7YXQb3QDthHU0zs0O3ueOfdyeFpXxTxxYDHoE6DKc2LL8fecizqRzvqv1s)

Design prompts

## Step 4 - Connect the LLM

Now, we need to instruct the [LLM](/vectorshift/platform/pipelines/large-language-models-llms) to be a chatbot for our product and connect the LLM to the above text prompt, then link an output node

1. We create a new [Text](/vectorshift/platform/pipelines/text) node with instructions for the LLM to be a chatbot for our product. To give more clarity to the model, we call out that it should be answering questions given `“Context”` (`“Context”` is what we labeled the data queried from the [Vector DB](/vectorshift/platform/pipelines/vector-db)). We connect this node to the `“system”` input node.

2. We connect the text box from above where we composed the prompt to the `“prompt”` node. Finally, we connect an output node to the `“response”` output node.

![](https://lh3.googleusercontent.com/GlNVINjuzNaH81jBwJMJjr31vJxMkyxcabGLIvq-wXKc6joT9CAAfMHiItpK_urJ3PcIooaUTmaNITjthzBzSEUDN9yCAGyKIxeJ0ORp2-lYDMSkK-Tz8q-ZNORpqYM9VnQeSPsItNA23fr5Dfvt1po)

Connect to LLM

Full pipeline (_click for full preview_)

![](https://lh4.googleusercontent.com/gV-bBZKFIAF0HUHKt0i14SYhSF49Ngx3pftFWB6ZTgM2WXxkkYLLmWw-OOQgAnIfgzqdKngANvLfxw8Q3c6PN4xwqE1PUXwa1lfo-8SZRHWJYM9R47qQt3rmvkIxtU1-PvHW6oD8lzJyKoTQO1zitPo)

Full pipeline

## Step 5 - Run and Share

Access the pipeline in a `“form”` type format. In the `“Pipelines”` tab, select the pipeline you just created and click `“run”`. Ask questions directly in the input box. 

Access the pipeline in chatbot format. In the `“Chatbot”` tab, click `“Add”.` In the popup and in the `“pipeline”` field, select the name of the pipeline that you just created. After creating the chatbot, click `“Run”` to start the chatbot. Chatbot can also be accessed via API (click on your profile on the top right and click settings to access the API key).

![](/images/CleanShot%202023-08-17%20at%2003.43.33%402x.png)

Finally, to publish your pipeline to the marketplace, go back to the `“Pipelines”` tab and find the pipeline you would like to publish. Then, click the three dots on the right-hand side and click `“Publish”`.