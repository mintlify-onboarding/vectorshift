---
title: "Document Search"
---

**Scenario: We want to build a pipeline that answers questions about a document in a chat-like format. In this case, we have questions about Apple’s annual report (> 80-page document with various charts) and want them quickly answered. We anticipate that we will have questions about other large documents and decide to build a pipeline to automate this workflow.** 

At a high level, we need to create the following pipeline components:

1. A way to feed files into the pipeline and embed them into a Vector database

2. A way for the LLM to receive 1) queries from the Vector Database, 2) store conversation history, and 3) receive a question from the user

3. A large language model instructed to be an analyst that answers questions based on a document

## Step 1 - Open the Pipeline Builder

Click “Build a Pipeline” within the “Overview” tab or click “Build” within the “Pipeline” tab. 

![](https://lh4.googleusercontent.com/HH2YJNvx1OoB_bfNbV1ISwpTQdS5V9BosDoDRWMm6DaoGGG75Juxtm743Vux6506Jrqdni3fr6gk2u3sBDkbyFpBhCRA6X_fxpBLrtHXPhfXszR10EhKfoZLVRatn6XqCZ_MT75MhQs6UqmPWRVpryc)

## Step 2 - Feeding the File

We need to create a structure that allows the pipeline to accept a file, feed the file into a Vector database, and allow for relevant information to be queried from the Vector database.

1. In the “files” tab, upload the document that you want to use by clicking the “Add” button

2. Creating the building blocks for the above action will have the following four node structures below  
   * [File Input](/vectorshift/platform/pipelines/file): select the file uploaded in step 1 with the pulldown in the file input node.  
   * [File Loader](/vectorshift/platform/pipelines/file#file-node): This converts the file into a format that the VectorDB can understand.  
   * [VectorDB Loader](/vectorshift/platform/pipelines/vector-db#vectordb-loader): loads the data into the Vector Database.  
   * [VectorDB Reader](/vectorshift/platform/pipelines/vector-db#vectordb-reader): provides an endpoint where the user can query the VectorDatabase.

Note: we connect the [VectorDB Loader](/vectorshift/platform/pipelines/vector-db#vectordb-loader) to the “`database`” node of the [VectorDB Reader](/vectorshift/platform/pipelines/vector-db#vectordb-reader).

![](https://lh3.googleusercontent.com/KLt1rkAG0dRKYAN8eOLRWWfcHfi_E7KZYsVhIhKsPejLJTFwwtw9dIQcnTqNvQInIvDV-JP0N0fnchZCDQRlvdUuOen3UWfNKeQaUiSc9oo2UQoY53jK-w3Hm_rYyrDRe9VuYa00AwmxIVRXXpYb3Ps)

## Step 3 - Create a Prompt

We need to create a prompt that contains the queries from the [VectorDB](/vectorshift/platform/pipelines/vector-db), store conversation history, and the question from the user:

1. The [Text](/vectorshift/platform/pipelines/text) node will serve as the “prompt” that will eventually prompt the LLM. Within this node, every time you use curly braces {}, the text within the braces will automatically appear on the left-hand side of the node; the associated data connected to the named nodes will “`replace`” the curly brackets when the pipeline runs.

2. The [Chat Memory ](/vectorshift/platform/pipelines/chat#chat-memory)node will allow the LLM to “remember” the previous conversation history. We connect it to the text node so that every time the text is passed to the LLM as a prompt, the previous conversation is also passed through.

3. The [Input](/vectorshift/platform/pipelines/inputs-outputs#input-nodes) node here will allow the user to ask questions. We need to connect it both to the VectorDB Reader node (“`query`” node) and the text node (prompt for the LLM). This is because:  
   * Passing the question into the [VectorDB Reader](/vectorshift/platform/pipelines/vector-db#vectordb-reader) will allow for the [VectorDB Reader](/vectorshift/platform/pipelines/vector-db#vectordb-reader) to query relevant information, which is outputted and also inputted into the prompt, and  
   * We need to pass the question into the prompt ([Text](/vectorshift/platform/pipelines/text) node) so the LLM receives the user input as well.

![](https://lh4.googleusercontent.com/e-3rpccHr3yNCMMzMc_Fj6ZMz9QjOPAr-uhv6Sa1jNxQIcJKvf-462934uKpOcsWUsiycrPd9eIOvgq-mI9PHpax9SyQP7YXQb3QDthHU0zs0O3ueOfdyeFpXxTxxYDHoE6DKc2LL8fecizqRzvqv1s)

Prompt design

## **Step 4 - Connect to LLM.** 

Here, we need to instruct the LLM to be an analyst and connect the LLM to the above text prompt, and link an [Output](/vectorshift/platform/pipelines/inputs-outputs#output-nodes) node:

1. We create a new [Text](/vectorshift/platform/pipelines/text) node with instructions for the LLM to be an analyst. To give more clarity to the model, we call out that it should be answering questions given “Context” (“Context” is what we labeled the data queried from the Vector DB). We connect this node to the “system” input node.

2. We connect the [Text](/vectorshift/platform/pipelines/text) node from above where we composed the prompt to the “`prompt`” node. Finally, we connect an output node to the “response” [Output](/vectorshift/platform/pipelines/inputs-outputs#output-nodes) node.

![](https://lh4.googleusercontent.com/5_XVt0s4jt9eX9dbJ6ycC2I0nRIIBT4xMMXSXElsAGG0x9TqzDBq4Cp645r-A2FqTpRwbvSoGOdOmivl4hUmaVEWF4dcczUOklSVIL3pdcSnzRSlsM3_0tsdGwYyAORhUC1KqHi7SK1OHH-LNg5DIwc)

Full Pipeline (click image to enlarge)

![](https://lh6.googleusercontent.com/IZ5nNnlFAV1HSusvM6FXlr_PzjZUXLXerqYn6DaEVbeiaRefba2pEPOMy9_bl3oKk-vXWW70QSCD8mkiyfmLNnAcOgIqsJp2XzBnKYOZANHeT_FhBMwwIP9Ja3c6ix9NOGa_PlhwV0OBhbgo-EoZAow)

## Step 5 - Run and Share

Access the pipeline in a “`form`” type format. In the “`Pipelines`” tab, select the pipeline you just created, and click “`run`”. Ask questions directly in the input box.

![](https://lh6.googleusercontent.com/HnYt8G3JifoG0LtWMRmeX4zmsRoh6Ctu15uQkScrD0iYxtNfidHbq4ue94tS3eWTn49FzKxNfCjHX--4UVGM4fxouHzdx8TJ7LOHDEt3H5pp3SNp87iGAefqn2nr0Is4h5T3VV_-CfBU4Gjc81X23zQ)

Run pipeline Document Analyst

![](https://lh6.googleusercontent.com/JktY1K6NNB9aGPfKIn5jhtvOOcL1Bb8aVg0eBGtXGwr3koLSz3Xj-np7RB_bkOMjGhbX0zNDeNhOVUfnc6hdnMQnZrmIJZXdfl_SvLaQ0qiOEIi7WeVTOvyNittOqfszUfWdgGcm5cVsTBXEBb6K_Pg)

Running pipeline interface

Access the pipeline in chatbot format. Click the “Chatbot” tab and click “`Add`”. In the popup and in the “`pipeline`” field, select the name of the pipeline that you just created. After creating the chatbot, click “`Run`” to start the chatbot. Chatbot can also be accessed via API (click on your profile on the top right and click settings to get your API key).

![](https://lh3.googleusercontent.com/E9pcgf6kYXgQNQdg9UEcQzVyRXUrXzgiIxQannXa5VzXdhXSlkAc6aBKqudwtM7SKeRrxLq9FaUVXXNaE7vGAdfkkE5g454aBsIYcI51lxKLiz-kFXblKuvvkzSajJMH-W2mLcM-DL7sTpFde4mJm1U)

Click "Add" to create a chatbot

![](https://lh5.googleusercontent.com/eHnAeZ86uyEXRPgmdYdYubVn7CHdHZgYHW2RA8J86RynG9C4_4adKibqt_dQZ_CuaqSzPBbJSgbGKM6OSA59zc9RG0sASpqfJCbTuZKUe9YgRbuUigqC45cYqKwlOu4Qe4fciL8fd2x6ulidxYO6OZo)

Chatbot result

Finally, to publish your pipeline to the marketplace, go back to the “`Pipelines`” tab and find the pipeline you would like to publish. Then, click the three dots on the right-hand side and click “`publish`”.