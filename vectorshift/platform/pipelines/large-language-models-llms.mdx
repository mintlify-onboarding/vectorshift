---
title: "Large Language Models (LLMs)"
description: "An LLM is a trained deep-learning model that can generate output in a human-like fashion."
---

We provide 3 types of LLM from OpenAI, each with its own maximum token input:

- GPT-3.5 has a max of 4096 tokens

- GPT-3.5-16k has a max of 16384 tokens

- GPT-4 has a max of 8192 tokens

<Frame>![](/images/6.webp)</Frame>

LLM model options

Notes:

1. Connect a text block to "system" to describe the AI's role (e.g., "You are a helpful financial assistant").  
   - It is sometimes helpful to provide an example of potential output (e.g, "For example, the output should look like...")  
   - Be as specific as possible - if the output should be one sentence or if the output should be in the first person, include the instructions in the text block connected to the system node.  
   - In the prompt (discussed below), it is sometimes helpful to label different parts of the prompt and call out those labels in the System (e.g., if you label relevant data from the VectorDB as "Context", you can ask the model to answer based on Context)

2. Connect a [Text](/vectorshift/platform/pipelines/text) node to "prompt" to include the prompt for the model.  
   - Common components that are useful in a prompt include a question (e.g., question - the question the user inputs) and context (e.g., relevant data returned from the Vector database)

3. The LLM's response will be outputted to the node connected to the "response" node.

4. If you are worried about your input to the LLM being too long, you can use [Vector DBs](/vectorshift/platform/pipelines/vector-db) or [Vector Stores](/vectorshift/platform/storage/vector-stores) to load your data and select only the most relevant parts.

Example of how to use LLM

<Frame>![](/images/7.webp)</Frame>